[
  {
    "objectID": "posts/strava-api-analysis/index.html",
    "href": "posts/strava-api-analysis/index.html",
    "title": "Analysing Strava data in R.",
    "section": "",
    "text": "With most of Australia closed between Christmas and New Years, I’ve found myself with some free time. This time for reflection has allowed me to consider potential goals for the coming year and, more importantly, my blasé attitude towards my physical health in the past year. 1\nI’ve always enjoyed having some physical goals for the year ahead, and some recent family health problems have reminded me of significance of prioritising one’s physical health.\nHowever, before setting specific goals I like to review my past training to help me understand where to start and how to map out the process of getting into better shape.\nSo, being the nerd that I am, I thought it would be fun to pull my running data from Strava using their API. Before getting started with this mini-project, I noticed that there wasn’t a clear pathway for R-users to obtain activity-level data from the Strava API in bulk - so hopefully this is useful to someone at some point! I am also somewhat underwhelmed by the analytic capabilities of the Strava app - especially if someone doesn’t want to pay $15/month in subscription fees!"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#introduction",
    "href": "posts/strava-api-analysis/index.html#introduction",
    "title": "Analysing Strava data in R.",
    "section": "",
    "text": "With most of Australia closed between Christmas and New Years, I’ve found myself with some free time. This time for reflection has allowed me to consider potential goals for the coming year and, more importantly, my blasé attitude towards my physical health in the past year. 1\nI’ve always enjoyed having some physical goals for the year ahead, and some recent family health problems have reminded me of significance of prioritising one’s physical health.\nHowever, before setting specific goals I like to review my past training to help me understand where to start and how to map out the process of getting into better shape.\nSo, being the nerd that I am, I thought it would be fun to pull my running data from Strava using their API. Before getting started with this mini-project, I noticed that there wasn’t a clear pathway for R-users to obtain activity-level data from the Strava API in bulk - so hopefully this is useful to someone at some point! I am also somewhat underwhelmed by the analytic capabilities of the Strava app - especially if someone doesn’t want to pay $15/month in subscription fees!"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#loading-libraries-and-registering-for-the-api",
    "href": "posts/strava-api-analysis/index.html#loading-libraries-and-registering-for-the-api",
    "title": "Analysing Strava data in R.",
    "section": "Loading libraries and registering for the API",
    "text": "Loading libraries and registering for the API\nBack to the task at hand, though! First, we’ll need to load the following libraries which will form the foundation for most of our wrangling and analysis.\n\n\nCode\n# A gentle reminder to call install.packages(\"package_name_here\") if you don't have any of these installed already!\n\nlibrary(tidyverse)\nlibrary(rStrava) \nlibrary(patchwork)\n\n\nNow, to access the Strava API. For this, you will need to register for this ‘service’ here 2.\nOnce you’ve done this, you will be prompted to name your app and to provide a website URL for it. For the URL, I just used my Strava athlete profile 3. After inputting this information, you should be directed to a page with your ‘client id’ and your ‘client secret’ - these are both necessary for accessing the API and, Strava reminds you, shouldn’t be shared with anyone!\n\n\nCode\napp_name &lt;- \"Put your app name here\"\nclient_id &lt;- \"Put your client id here\"\nclient_secret &lt;- \"Put your client secret here\"\n\n\nThen, with this information we can create an API request! I’ll first create an object that we can call any time we need to make a request.\n\n\nCode\nstoken &lt;- httr::config(\n  token = strava_oauth(\n    app_name, \n    client_id, \n    client_secret, \n    app_scope = \"activity:read_all\",\n    cache = TRUE)\n  )"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#inspiration",
    "href": "posts/strava-api-analysis/index.html#inspiration",
    "title": "Analysing Strava data in R.",
    "section": "Inspiration",
    "text": "Inspiration\nBefore moving forward I want to acknowledge that part of my motivation for re-engaging with the Strava API comes from Ilya Kashnitsky’s beautiful heatmap, pictured below. He created it by taking aggregate-level polylines from each of his activities and plotting them on an inset map - pretty cool and the code for it can be found here!\n\n\n\nA really cool colour-inverted heatmap from Ilya’s Strava runs!"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#function-time",
    "href": "posts/strava-api-analysis/index.html#function-time",
    "title": "Analysis of Strava data - and training load - in R.",
    "section": "Function time!",
    "text": "Function time!\nHowever, getting the data for each of the 411 activities I have on Strava using this process would take some time. So, let’s automate it!\nFirst, let’s create a smaller version of the dataframe to make sure that we don’t have any errors before bombarding the API with bad requests.\n\n# A smaller version of our dataset\n\nmini &lt;- head(activity.list, 5)\n\n# Run a function from rows 1:5 of the mini dataframe, using the activity 'id' column as our index. \n\ndata &lt;- lapply(1:nrow(mini), function(i) {\n  \n  temp &lt;- get_streams(stoken, id = mini$id[i])\n  \n})\n\nNow, already this throws up an error message which isn’t too informative:\n\nError in get_streams(stoken, id = mini$id[i]) : Not Found (HTTP 404).\n\nFrom this, there’s no way to know which of the 5 activities caused this problem. Herein lies the opportunity to use a command that I’ve found myself relying on a lot lately: tryCatch(). This function allows for two main possibilities:\n\nThe omission of data causing an error, allowing the command to run without halting.\nUnderstanding which cases are causing the problem.\n\nIn this instance, I want to understand what’s going wrong with the non-functional ‘id’ variables.\n\ndata &lt;- lapply(1:nrow(mini), function(i) {\n\n  # attempts to run this function\n    \n  tryCatch({\n    \n    temp &lt;- get_streams(stoken, id = mini$id[i])\n    \n  },\n  \n  # if an error occurs, provides the following output\n  \n  error = function(error) {\n    \n    cat(\"Error at index:\", i, \"\\n\")\n    cat(\"Error message:\", conditionMessage(error), \"\\n\")\n    return(NULL)  \n    \n  }\n  )\n}) \n\nError at index: 4 \nError message: Not Found (HTTP 404). \nError at index: 5 \nError message: Not Found (HTTP 404). \n\n\n\nOutstanding! We now have the data from the first three activities, but we’re also able to see that activities 4 and 5 in this dataframe were causing the error. A brief inspection shows that the error is likely due to these activities being manually uploaded - a symptom of a broken Garmin watch!\n\n# Look at a valid row (3) and an invalid row (4)\n\nmini[c(3,4),] %&gt;%\n  select(id, manual, max_speed, start_latlng1)\n\n\n\n  \n\n\n\n\nTo confirm this hypothesis, we can run the function over all manually uploaded activities to see if any of them are able to produce a result.\n\nmanual_test &lt;- activity.list %&gt;%\n  filter(manual == \"TRUE\")\n\ndata &lt;- lapply(1:nrow(manual_test), function(i) {\n  \n  tryCatch({\n    \n    temp &lt;- get_streams(stoken, id = manual_test$id[i])\n    \n  },\n  \n  error = function(error) {\n    \n    paste(\"Error at index:\", i, \"\\n\")\n    paste(\"Error message:\", conditionMessage(error), \"\\n\")\n    return(NULL)  \n    \n  }\n  )\n}) %&gt;%\n  bind_rows()\n\nhead(data)\n\n\n\n  \n\n\n\n\nNope. Well, that was an easy fix! Now, let’s create a dataframe which omits these activities.\n\nactivities &lt;- activity.list %&gt;%\n  filter(manual != \"TRUE\") %&gt;%\n  filter(!is.na(start_latlng1),\n         !is.na(end_latlng1))\n\nmini.1 &lt;- head(activities, 3)\n\n# Run a function from rows 1:5 of the mini dataframe, using the activity 'id' column as our index. \n\ndata &lt;- lapply(1:nrow(mini.1), function(i) {\n  \n  temp &lt;- get_streams(stoken, id = mini.1$id[i])\n  \n})"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Analysing Strava data in R.\n\n\n\n\n\n\n\nR\n\n\nData analysis\n\n\nTutorial\n\n\nStrava\n\n\nSports statistics\n\n\n\n\nA former runner wrangles data through the Strava API.\n\n\n\n\n\n\nDec 29, 2023\n\n\nLuke Ashton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luke Ashton",
    "section": "",
    "text": "I’m a PhD student at Macquarie University in Sydney, currently working on research investigating educational inequality in Australia.\nHowever, this blog will largely be a space for me to write about things that I work on in my spare time - and the topics covered will be quite wide-ranging.\nAs the year comes to an end, and I find myself with more time, I am considering my health and fitness more than I have for some time. As a result, some of the first few articles will be based on some health/fitness data."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Luke Ashton",
    "section": "",
    "text": "I’m a PhD student at Macquarie University in Sydney, currently working on research investigating educational inequality in Australia.\nHowever, this blog will largely be a space for me to write about things that I work on in my spare time - and the topics covered will be quite wide-ranging.\nAs the year comes to an end, and I find myself with more time, I am considering my health and fitness more than I have for some time. As a result, some of the first few articles will be based on some health/fitness data."
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#request-limits",
    "href": "posts/strava-api-analysis/index.html#request-limits",
    "title": "Analysis of Strava data - and training load - in R.",
    "section": "Request limits",
    "text": "Request limits\nHowever, the next problem we have is with the Strava API itself. See below:\n\n\n\nRequest limits can be a problem when you don’t pay for access.\n\n\nGiven that my Strava account has 320 activities of interest, some more work will need to go into creating a function that can handle this limitation. Ideally such a function would create a dataframe from a list of activities, go to sleep for 15 minutes, and then request the remaining activities. 3\nI’ve had a play around with this, and this code should work for any number of activities - however, be mindful that this function will take 15 minutes per 190 activities4!\n\n\nCode\nresult_list &lt;- list()\nbatches &lt;- (nrow(activities) %/% 190) + 1\ncases_per_batch &lt;- nrow(activities) / batches\n\n\nstart &lt;- Sys.time()\n\nfor (i in 1:nrow(activities)) {\n  Sys.sleep(if (i %% cases_per_batch == 1 & \n                i &gt; 1) 900 else 0)\n\n  temp &lt;- get_streams(stoken, activities$id[i])\n\n  data &lt;- temp %&gt;%\n    purrr::transpose() %&gt;%\n    tibble::as_tibble() %&gt;%\n    dplyr::select(type, data) %&gt;%\n    dplyr::mutate(type = unlist(type),\n                  data = purrr::map(data, ~replace(.x, length(.x) == 0, NA))) %&gt;%\n    tidyr::spread(data = ., key = type, value = data) %&gt;%\n    tidyr::unnest(cols = everything()) %&gt;%\n    dplyr::mutate_at(vars(-latlng), ~ unlist(.)) %&gt;%\n    tidyr::unnest_wider(latlng, names_sep = \"_\") %&gt;%\n    dplyr::mutate(activity = i,\n                  id = activities$id[i])\n  \n  result_list[[i]] &lt;- data\n}\n\nend &lt;- Sys.time()\n\ndf &lt;- result_list %&gt;%\n  bind_rows() \n\n# Now, I'm going to save this dataset, so I don't have to run this call again and waste 16 minutes of my life!\n# save(df, file = \"data/all_activities.RData\")"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#aggregate-level-data",
    "href": "posts/strava-api-analysis/index.html#aggregate-level-data",
    "title": "Analysis of Strava data - and training load - in R.",
    "section": "Aggregate-level data",
    "text": "Aggregate-level data\nHowever, one limitation of the standard API and the RStrava package is that most of the simpler functions only provide aggregated activity data. For instance, the following code creates an activity list with some topline stats from the 411 activities I’ve logged with Strava.\n\n\nCode\nactivity.list &lt;- stoken %&gt;%\n  get_activity_list() %&gt;%\n  compile_activities()\n\nhead(activity.list)\n\n\n\n\n  \n\n\n\nWhile it’s interesting to see items like the achievement count, average speed, and average heart rate for each activity,this information doesn’t necessarily help us understand how training load affects the body. To get a better intuition for this, consider two workouts: one a high-intensity sprint session focusing on 400m intervals with a long recovery; the other a typical easy ‘Zone 2’ aerobic workout. These two workouts could, in theory, be of the same duration, cover the same amount of distance, and even end up having similar average heart rates - but their training effects, and subsequently the toll they take on the body, are markedly different.\nTherefore, I want activity-level data - which is typically collected at the interval of 1 second from most sports watches."
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#activity-level-data",
    "href": "posts/strava-api-analysis/index.html#activity-level-data",
    "title": "Analysing Strava data in R.",
    "section": "Activity-level data",
    "text": "Activity-level data\nFortunately, there’s a command that allows us to access this information.\n\n\nCode\nget_streams(stoken,\n            id = \"activity id here\")\n\n\nHowever, getting the data for each of the 412 activities I have on Strava using this process would take some time. So, let’s automate it!\nFirst, let’s create a smaller version of the dataset to make sure that we don’t have any errors before bombarding the API with bad requests.\n\n\nCode\n# A smaller version of our dataset\n\nmini &lt;- head(activity.list, 5)\n\n# Run a function from rows 1:5 of the mini dataframe, using the activity 'id' column as our index. \n\ndata &lt;- lapply(1:nrow(mini), function(i) {\n  \n  temp &lt;- get_streams(stoken, id = mini$id[i])\n  \n})\n\n\nFortunately we did this with a smaller dataset, because - using my data, at least - there’s already an error!\n\n\nCode\nError in get_streams(stoken, id = mini$id[i]) : Not Found (HTTP 404).\n\n\nA problem with this function is that there’s no way to know which of the 5 activities we requested caused this problem. Although, this provides us with the opportunity to use a command that I’ve found myself relying on a lot lately: tryCatch(). This function allows for two main possibilities:\n\nThe omission of data causing an error, allowing the command to run without halting.\nUnderstanding which cases are causing the problem.\n\nIn this instance, I want to understand what’s going wrong with the non-functional ‘id’ variables to make sure that there aren’t problems in the future and to prevent sending a tonne of bad requests to the API.\n\n\nCode\ndata &lt;- lapply(1:nrow(mini), function(i) {\n\n  # 'tries' to run this function\n    \n  tryCatch({\n    \n    temp &lt;- get_streams(stoken, id = mini$id[i])\n    \n  },\n  \n  # if an error occurs, provides the following output\n  \n  error = function(error) {\n    \n    cat(\"Error at index:\", i, \"\\n\")\n    cat(\"Error message:\", conditionMessage(error), \"\\n\")\n    return(NULL)  \n    \n  }\n  )\n}) \n\n\nError at index: 5 \nError message: Not Found (HTTP 404). \n\n\nOutstanding! We now have the data from the first four activities, but we’re also able to see that activity 5 in this dataframe is causing the error. A brief inspection shows that the error is likely due to these activities being manually uploaded - a symptom of a broken Garmin watch, and missing several key variables of importance!\n\n\nCode\n# Look at a valid row (4) and an invalid row (5)\n\nmini[c(4,5),] %&gt;%\n  select(id, manual, max_speed, start_latlng1)\n\n\n\n\n  \n\n\n\nTo confirm this hypothesis, we can run the function over some manually uploaded activities to see if any of them are able to produce a valid result.\n\n\nCode\nmanual_test &lt;- activity.list %&gt;%\n  filter(manual == \"TRUE\") %&gt;%\n  head(., 5)\n\ndata &lt;- lapply(1:nrow(manual_test), function(i) {\n  \n  tryCatch({\n    \n    temp &lt;- get_streams(stoken, id = manual_test$id[i])\n    \n  },\n  \n  error = function(error) {\n    \n    paste(\"Error at index:\", i, \"\\n\")\n    paste(\"Error message:\", conditionMessage(error), \"\\n\")\n    return(NULL)  \n    \n  }\n  )\n}) %&gt;%\n  bind_rows()\n\nhead(data)\n\n\n\n\n  \n\n\n\n\nNot a single valid result. Well, at least that was an easy fix! Now, let’s create a dataframe which omits these activities, and to be proactive let’s remove activities without a set of latitude/longitude coordinates and heart rate data (as we might want to explore these later on), and any activities that aren’t runs.4\n\n\nCode\nactivities &lt;- activity.list %&gt;%\n  filter(manual != \"TRUE\") %&gt;%\n  filter(!is.na(start_latlng1),\n         !is.na(max_heartrate)) %&gt;%\n  filter(grepl(\"Run\", sport_type))\n\nmini.1 &lt;- head(activities, 5)\n\n# Run a function from rows 1:5 of the mini dataframe, using the activity 'id' column as our index. \n\ndata &lt;- lapply(1:nrow(mini.1), function(i) {\n  \n  temp &lt;- get_streams(stoken, id = mini.1$id[i])\n  \n})"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#wrangling-the-data",
    "href": "posts/strava-api-analysis/index.html#wrangling-the-data",
    "title": "Analysis of Strava data - and training load - in R.",
    "section": "Wrangling the data",
    "text": "Wrangling the data\nWe have now - in theory - some working code to pull activity-level data from Strava’s API. Let’s see what one of the elements from our dataset looks like.\n\n\nCode\ntemp &lt;- get_streams(stoken, id = mini.1$id[1])\n\nsummary(temp)\n\n\n      Length Class  Mode\n [1,] 5      -none- list\n [2,] 5      -none- list\n [3,] 5      -none- list\n [4,] 5      -none- list\n [5,] 5      -none- list\n [6,] 5      -none- list\n [7,] 5      -none- list\n [8,] 5      -none- list\n [9,] 5      -none- list\n[10,] 5      -none- list\n\n\nHmmm - that’s not a particularly nice structure for analysis, so let’s tidy it up a little. First, we’ll transpose the set of lists and convert them to a tibble. 5\n\n\nCode\ntemp %&gt;%\n  purrr::transpose() %&gt;%\n  tibble::as_tibble()\n\n\n\n\n  \n\n\n\nThat’s a bit better, now at least we can begin to work with this data! The first step is keep the type and data columns and remove everything else, then we should unlist the data and convert 0-length lists to NA values. After this, we can perform some more general tidying and include some important variables to make these activities identifiable in the future.\n\n\nCode\ntemp.data &lt;- temp %&gt;%\n  purrr::transpose() %&gt;%\n  tibble::as_tibble() %&gt;%\n  dplyr::select(type, data) %&gt;%\n  dplyr::mutate(type = unlist(type),\n                data = purrr::map(data, ~ purrr::modify_if(.x, ~length(.) == 0, ~NA))) %&gt;%\n  \n  # With the data tidied, it's time to wrangle and turn it into a tidy format!\n  \n  tidyr::spread(data = ., key = type, value = data) %&gt;%\n  tidyr::unnest(cols = everything()) %&gt;%\n  dplyr::mutate_at(vars(-latlng), ~ unlist(.)) %&gt;%\n  tidyr::unnest_wider(latlng, names_sep = \"_\") %&gt;%\n  dplyr::mutate(activity = 1,\n                id = mini$id[1])\n\nhead(temp.data)\n\n\n\n\n  \n\n\n\nPerfect! We now have that all-important heart rate data, along with a few other useful variables for some later analysis!"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#footnotes",
    "href": "posts/strava-api-analysis/index.html#footnotes",
    "title": "Analysing Strava data in R.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nor perhaps the past few years…↩︎\nThis assumes that you already have a Strava account.↩︎\nI assume this is a requirement intended for the more serious app developers, rather than the hobbyist analysts.↩︎\nIt appears that I have a few cross country ski activities and hikes in here!↩︎\nI’m going to call each library in this tidying step, as we’ll be incorporating this code into a bigger function later on.↩︎\nI use 180 requests instead of 200 to allow for some wiggle room, in case you need to make some changes to your code, or want to make another small request.↩︎\nThis can be calculated using the following formula: \\[ ((heart\\:rate - resting\\:heart\\:rate) / (max\\:heart\\:rate - resting\\:heart\\:rate)) \\]↩︎\nThis can be calculated using the following formula: \\[ TRIMP = (d.time / 60) * HRR * (0.64e ^{1.92 * HRR} ) \\]↩︎"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#creating-a-measure-of-training-load",
    "href": "posts/strava-api-analysis/index.html#creating-a-measure-of-training-load",
    "title": "Analysing Strava data in R.",
    "section": "Creating a measure of training load!",
    "text": "Creating a measure of training load!\nFor quantifying training load, let’s use a modified version of the Training Impulse (TRIMP) formula (see Morton, Fitz-Clarke, and Banister 1990). This version incorporates an individual weighting factor and accounts for different resting and maximum heart rates. It also has a non-linear term to try to capture the phenomena that higher heart rates are more fatiguing / indicative of a higher training load.\nAlso, as mentioned before, rather than use the average heart rate method, we’ll create a measure for each observation of heart rate data. This means that we will need to create a few new variables in our dataset.\n\nthe change in time between observations - (d.time)\nthe athlete’s resting and maximum heart rates\nthe specific heart rate reserve (HRR) for each heart rate measure 7\nthe TRIMP value for each heart rate observation, converted to a value per minute 8\nsome values for day, week, month, and year\n\nWe can also use the activity ‘id’ variable to include some of the aggregated data for each activity.\n\n\nCode\n# Select aggregated activity variables\n\nactivity.variables &lt;- activity.list %&gt;%\n  select(id, total_elevation_gain, \n         activity_distance = distance,\n         average_speed, max_speed,\n         date = start_date_local)\n\ndf1 &lt;- df %&gt;%\n  arrange(activity, time) %&gt;%\n  left_join(activity.variables) %&gt;%\n  group_by(activity) %&gt;%\n  dplyr::mutate(activity_time = max(time),\n                d.time = time - dplyr::lag(time),\n                d.time = ifelse(is.na(d.time), 0, d.time),\n                rest_hr = 55,\n                max_hr = 190,\n                HRR = ((heartrate - rest_hr) / (max_hr - rest_hr)),\n                TRIMP = (d.time / 60) * HRR * (0.64*exp(1))^(1.92 * HRR),\n                TRIMP_session = sum(TRIMP, na.rm = T),\n                year = year(as.POSIXct(date)),\n                month = month(as.POSIXct(date)),\n                day = weekdays(as.POSIXct(date)),\n                week = isoweek(ymd_hms(date)))"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#plotting",
    "href": "posts/strava-api-analysis/index.html#plotting",
    "title": "Analysis of Strava data - and training load - in R.",
    "section": "Plotting",
    "text": "Plotting\nFirst, let’s create a modified version of the dataset to provide some general insights.\n\n\nCode\nplotting_data &lt;- df1 %&gt;%\n  select(activity, date, activity_distance, \n         average_speed, max_speed, activity_time, \n         TRIMP_session, year, month, day, week,\n         ymd) %&gt;%\n  unique()\n\n\n\n\nCode\nplotting_data %&gt;%\n  group_by(week) %&gt;%\n  dplyr::mutate(weekly_TRIMP = sum(TRIMP_session, na.rm = T)) %&gt;%\n\n    \n  ggplot(aes(x = week, y = weekly_TRIMP, fill = factor(year))) + \n  geom_col() + \n  scale_fill_manual(values = ggokabeito::palette_okabe_ito()) + \n  theme_minimal()  +\n  theme(\n    text = element_text(family = \"Consolas\"),\n    legend.position = \"none\",\n    legend.title = element_blank()\n  ) + \n  labs(title = \"Weekly TRIMP across 2018 and 2019\",\n       x = \"Week of the year\",\n       y = \"Weekly TRIMP\",\n       caption = \"My consistency is sorely lacking.... where's 2020 & 2022?\") + \n  facet_wrap(~ year, ncol = 1)\n\n\n\n\n\n\n\nCode\nplot_trimp &lt;- plotting_data %&gt;%\n  filter(year == 2018 | \n           year == 2019) %&gt;%\n  group_by(year, week) %&gt;%\n  dplyr::summarise(weekly_TRIMP = sum(TRIMP_session, na.rm = T)) %&gt;%\n\n    \n  ggplot(aes(x = week, y = weekly_TRIMP, fill = factor(year))) + \n  geom_col() + \n  scale_fill_manual(values = ggokabeito::palette_okabe_ito(c(1,2))) + \n  coord_cartesian(xlim = c(0, 52)) + \n  theme_minimal()  +\n  theme(\n    text = element_text(family = \"Consolas\"),\n    legend.position = \"none\",\n    legend.title = element_blank()\n  ) + \n  labs(title = \"Weekly TRIMP across 2018 and 2019\",\n       x = \"Week of the year\",\n       y = \"Weekly TRIMP\",\n       caption = \"My consistency is sorely lacking.\") + \n  facet_wrap(~ year)\n\nplot_trimp\n\n\n\n\n\n\n\nCode\nplot_speed &lt;- plotting_data %&gt;%\n  filter(year == 2018 | \n           year == 2019) %&gt;%\n  group_by(year, week) %&gt;%\n  dplyr::summarise(weekly_TRIMP = sum(TRIMP_session, na.rm = T),\n                   average_speed = mean(average_speed, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n\n    \n  ggplot(aes(x = week, y = average_speed, colour = factor(year))) + \n  geom_point() + \n  scale_colour_manual(values = ggokabeito::palette_okabe_ito(c(1,2))) + \n  coord_cartesian(xlim = c(0, 52)) + \n  theme_minimal()  +\n  theme(\n    text = element_text(family = \"Consolas\"),\n    legend.position = \"none\",\n    legend.title = element_blank()\n  ) + \n  labs(title = \"Average weekly speed of runs across 2018 and 2019\",\n  x = \"Week of the year\",\n  y = \"Weekly average speed (km/h)\",\n  caption = \"My consistency is sorely lacking.\") +\n  facet_wrap(~ year)\n\nplot_speed\n\n\n\n\n\n\n\nCode\nlibrary(patchwork)\n\nplot_trimp + plot_speed + plot_layout(ncol = 1)"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#api-request-limits",
    "href": "posts/strava-api-analysis/index.html#api-request-limits",
    "title": "Analysing Strava data in R.",
    "section": "API Request limits",
    "text": "API Request limits\nWe’re almost there, we just need to ask the API for this data and combine it. However, the next problem we face comes from Strava’s API architecture:\n\n\n\nAPI Request limits can be a problem when you don’t pay-to-play.\n\n\nAfter filtering my activities, I still have to make 308 requests through the API, which means that we’ll have to create a function which knows to stop sending requests for a 15-minute period once we approach that 200 request limit, and once that time has passed can resume with the remaining requests.\nIdeally, this function would be able to work with a request of any number of items, not just my 308. I’ve created something that should work for this task using a modulus operator and a Sys.sleep command. If you’re going to run it yourself, be mindful that this function will take 15 minutes per 180 activities6!\n\n\nCode\n# First, create an empty list that will store each request\nresult_list &lt;- list()\nbatches &lt;- (nrow(activities) %/% 180) + 1\ncases_per_batch &lt;- nrow(activities) / batches\n\n\nfor (i in 1:nrow(activities)) {\n  Sys.sleep(if (i %% cases_per_batch == 1 & \n                i &gt; 1) 900 else 0)\n\n  temp &lt;- get_streams(stoken, activities$id[i])\n\n  data &lt;- temp %&gt;%\n    purrr::transpose() %&gt;%\n    tibble::as_tibble() %&gt;%\n    dplyr::select(type, data) %&gt;%\n    dplyr::mutate(type = unlist(type),\n                  data = purrr::map(data, ~replace(.x, length(.x) == 0, NA))) %&gt;%\n    tidyr::spread(data = ., key = type, value = data) %&gt;%\n    tidyr::unnest(cols = everything()) %&gt;%\n    dplyr::mutate_at(vars(-latlng), ~ unlist(.)) %&gt;%\n    tidyr::unnest_wider(latlng, names_sep = \"_\") %&gt;%\n    dplyr::mutate(activity = i,\n                  id = activities$id[i])\n  \n  result_list[[i]] &lt;- data\n}\n\ndf &lt;- result_list %&gt;%\n  bind_rows() \n\n# Now, I'm going to save this dataset, so I don't have to run this call again and waste 16 minutes of my life!\n# save(df, file = \"data/all_activities.RData\")"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#aggregated-activity-data",
    "href": "posts/strava-api-analysis/index.html#aggregated-activity-data",
    "title": "Analysing Strava data in R.",
    "section": "Aggregated activity data",
    "text": "Aggregated activity data\nHowever, one limitation of the standard API and the RStrava package is that most of the simpler functions only provide aggregated activity data. For instance, the following code creates an activity list with some topline stats from the 412 activities I’ve logged with Strava.\n\n\nCode\nactivity.list &lt;- stoken %&gt;%\n  get_activity_list() %&gt;%\n  compile_activities()\n\nhead(activity.list)\n\n\n\n\n  \n\n\n\nWhile it’s interesting to see items like the achievement count, average speed, and average heart rate for each activity, this information may not help us get an accurate measure of training load. One of the more well-known measures of training load is the TRIMP (training impulse) value, which estimates training load by multiplying the number of minutes the activity took by the average heart rate during it.\nTo get a better intuition for why I believe activity-level data to be important for measuring training load, consider two workouts: one a high-intensity sprint session focusing on 400m intervals with a long recovery; the other a typical easy ‘Zone 2’ aerobic workout. These two workouts could, in theory, be of the same duration, cover the same amount of distance, and even end up having similar average heart rates - but their training effects, and subsequently the toll they take on the body, are markedly different.\nTherefore, I want activity-level data - which is typically collected at the interval of 1 second from most sports watches."
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#wrangling-and-tidying-the-data",
    "href": "posts/strava-api-analysis/index.html#wrangling-and-tidying-the-data",
    "title": "Analysing Strava data in R.",
    "section": "Wrangling and tidying the data",
    "text": "Wrangling and tidying the data\nWe have now - in theory - some working code to pull activity-level data from Strava’s API. Let’s see what one of the elements from our dataset looks like.\n\n\nCode\ntemp &lt;- get_streams(stoken, id = mini.1$id[1])\n\nsummary(temp)\n\n\n      Length Class  Mode\n [1,] 5      -none- list\n [2,] 5      -none- list\n [3,] 5      -none- list\n [4,] 5      -none- list\n [5,] 5      -none- list\n [6,] 5      -none- list\n [7,] 5      -none- list\n [8,] 5      -none- list\n [9,] 5      -none- list\n[10,] 5      -none- list\n\n\nHmmm - that’s not a particularly nice structure for analysis, so let’s tidy it up a little. First, we’ll transpose the set of lists and convert them to a tibble. 5\n\n\nCode\ntemp %&gt;%\n  purrr::transpose() %&gt;%\n  tibble::as_tibble()\n\n\n\n\n  \n\n\n\nThat’s a bit better, now at least we can begin to work with this data! The next step is keep the ‘type’ and ‘data’ columns and remove everything else, then we should unlist the data and convert 0-length lists to NA values. After this, we can perform some more tidying and add some important variables to make these activities identifiable in the future.\n\n\nCode\ntemp.data &lt;- temp %&gt;%\n  purrr::transpose() %&gt;%\n  tibble::as_tibble() %&gt;%\n  dplyr::select(type, data) %&gt;%\n  dplyr::mutate(type = unlist(type),\n                data = purrr::map(data, ~ purrr::modify_if(.x, ~length(.) == 0, ~NA))) %&gt;%\n  \n  # With the data wrangled, let's transform it into a tidy format!\n  \n  tidyr::spread(data = ., key = type, value = data) %&gt;%\n  tidyr::unnest(cols = everything()) %&gt;%\n  dplyr::mutate_at(vars(-latlng), ~ unlist(.)) %&gt;%\n  tidyr::unnest_wider(latlng, names_sep = \"_\") %&gt;%\n  dplyr::mutate(activity = 1,\n                id = mini$id[1])\n\nhead(temp.data)\n\n\n\n\n  \n\n\n\nPerfect! We now have that all-important heart rate data, along with a few other useful variables for some later analysis!"
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#plotting-our-data",
    "href": "posts/strava-api-analysis/index.html#plotting-our-data",
    "title": "Analysing Strava data in R.",
    "section": "Plotting our data",
    "text": "Plotting our data\nFirst, let’s create a modified version of the dataset to provide some general insights. Now that we have a more accurate version of activity-level training load, we can re-aggregate the data for plotting.\n\n\nCode\nplotting_data &lt;- df1 %&gt;%\n  select(activity, date, activity_distance, \n         average_speed, max_speed, activity_time, \n         TRIMP_session, year, month, day, week) %&gt;%\n  unique()\n\n\nOkay, now let’s create a plot to look at our training load over the years we have data available for. This will allow us to assess our consistency and see trends in our training.\n\n\nCode\nplotting_data %&gt;%\n  group_by(year, week) %&gt;%\n  dplyr::summarise(weekly_TRIMP = sum(TRIMP_session, na.rm = T)) %&gt;%\n\n    \n  ggplot(aes(x = week, y = weekly_TRIMP, fill = factor(year))) + \n  geom_col() + \n  scale_fill_manual(values = ggokabeito::palette_okabe_ito()) + \n  theme_minimal()  +\n  theme(\n    legend.position = \"none\",\n    legend.title = element_blank()\n  ) + \n  labs(title = \"Weekly training load (TRIMPexp) over the years\",\n       x = \"Week of the year\",\n       y = \"Weekly training load\") + \n  facet_wrap(~ year, ncol = 1, scales = \"fixed\")\n\n\n\n\n\nWell, that’s not great… where’s 2020 and 2022? In my defence, I did run throughout the back half of 2023 but didn’t record these sessions because my Garmin watch broke, and I haven’t gotten around to having it fixed yet. So that’s a good lesson: you can’t analyse or evaluate data that you don’t have!\nPerhaps, it would be more useful to look at the data from 2018 and the start of 2019, where I was somewhat more consistent with my training. First, let’s re-plot the weekly training load data from 2018 and 2019, and then we can look at the average speed of all workouts each week during this time period, too. It might also be fun to use a slightly different ‘geom’ for each plot.\n\n\nCode\nplot_trimp &lt;- plotting_data %&gt;%\n  filter(year == 2018 | \n           year == 2019) %&gt;%\n  group_by(year, week) %&gt;%\n  dplyr::summarise(weekly_TRIMP = sum(TRIMP_session, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n\n    \n  ggplot(aes(x = week, y = weekly_TRIMP, fill = factor(year))) + \n  geom_col() + \n  scale_fill_manual(values = ggokabeito::palette_okabe_ito(c(1,2))) + \n  coord_cartesian(xlim = c(0, 52)) + \n  theme_minimal()  +\n  theme(\n    legend.position = \"none\",\n    legend.title = element_blank()\n  ) + \n  labs(title = \"Weekly TRIMP across 2018 and 2019\",\n       x = \"Week of the year\",\n       y = \"Weekly training load\") + \n  facet_wrap(~ year)\n\nplot_trimp\n\n\n\n\n\n\n\nCode\nplot_speed &lt;- plotting_data %&gt;%\n  filter(year == 2018 | \n           year == 2019) %&gt;%\n  group_by(year, week) %&gt;%\n  dplyr::summarise(weekly_TRIMP = sum(TRIMP_session, na.rm = T),\n                   average_speed = mean(average_speed, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n\n    \n  ggplot(aes(x = week, y = average_speed, colour = factor(year))) + \n  geom_point() + \n  scale_colour_manual(values = ggokabeito::palette_okabe_ito(c(1,2))) + \n  coord_cartesian(xlim = c(0, 52)) + \n  theme_minimal()  +\n  theme(\n    legend.position = \"none\",\n    legend.title = element_blank()\n  ) + \n  labs(title = \"Average weekly speed of runs across 2018 and 2019\",\n  x = \"Week of the year\",\n  y = \"Weekly average speed (km/h)\") +\n  facet_wrap(~ year)\n\nplot_speed\n\n\n\n\n\nIt looks like there’s a relationship between consistent training and the average weekly speed of workouts. Hmmmm, who would have thought? However, to make this a little bit clearer we can plot these two figures together using the patchwork library.\n\n\nCode\ncombined_plot &lt;- plot_trimp + plot_speed + plot_layout(ncol = 1)\n\ncombined_plot\n\n\n\n\n\nCode\n# Let's save this to use as the image for the post! \n\n# ggsave(combined_plot, file = \"images/combined_plot.png\",\n#        height = 6, width = 8, units = \"in\")\n\n\nNot bad. While there’s plenty more analysis to undertake, this post is already much longer than anticipated due to the intricacies of working with the Strava API, so I think I’ll leave it here for now and dive into this data in more detail later."
  },
  {
    "objectID": "posts/strava-api-analysis/index.html#takeaways-goals-for-the-new-year",
    "href": "posts/strava-api-analysis/index.html#takeaways-goals-for-the-new-year",
    "title": "Analysing Strava data in R.",
    "section": "Takeaways & goals for the New Year",
    "text": "Takeaways & goals for the New Year\nSo, it’s clear that I’ve been terribly inconsistent with my aerobic training. However, I don’t record data for my resistance training anymore, so there’s no way to tell how inconsistent I’ve been with that. Although, I’ve probably been training with weights, on average, twice per week for the past few years.\nFor 2024, I have ideas of an ironman distance triathlon and my first 100 miler run floating around in my mind, but let’s start with the basics: consistency. Perhaps 3 resistance training sessions per week and building up to a few hours of aerobic training each week across some runs, a cycle, and a swim."
  }
]